{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport torch \nimport torch.nn as nn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T22:27:05.066274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some Helper Functions","metadata":{}},{"cell_type":"code","source":"def printUniqueCounts(df, columnName: str = None):\n    column = None\n    if columnName is None:\n        column = df\n    else:\n        column = df[columnName]\n    titles = column.unique()\n    counts = []\n    for title in titles:\n        if pd.isna(title):\n            count = sum(column.isnull())\n        else:\n            count = sum(column==title)\n        counts.append([title,count])\n    \n    counts.sort(key=lambda x: -x[1])\n    \n    for item in counts:\n        print(str(item[0])+\": \"+str(item[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData = pd.read_csv('/kaggle/input/titanic/train.csv')\nprint(trainData.shape)\ntrainData.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to clean up the data for use with our ML algorithm and do some feature engineering.\n\nFirst lets see which columns have NaN / null values:","metadata":{}},{"cell_type":"code","source":"for column in trainData.columns:\n    empty = len(trainData[trainData[column].isnull()])\n    if empty>0: print(column + \" has \" + str(empty) + \" missing data points\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fill in missing age values with average age for now. In the future we may want to infer the age from the other columns, but for now this will have to suffice.","metadata":{}},{"cell_type":"code","source":"#fill in missing age values with average age\navgAge = trainData[\"Age\"].mean()\nprint(\"Average age: \")\ntrainData[\"Age\"].fillna(avgAge)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To gain a better idea of how to handle the missing values in embarked, lets first see how many people embarked from each port:","metadata":{}},{"cell_type":"code","source":"printUniqueCounts(trainData,\"Embarked\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only 3 ports of embarcation, which makes this feature a good candidate for one-hot encoding. Missing values will simply be recorded as 'False' in all embarcation ports","metadata":{}},{"cell_type":"code","source":"trainData = pd.get_dummies(trainData,columns=[\"Embarked\"])\ntrainData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The remaining column with missing data is Cabin. This is really two features: cabin area (the letter) and cabin number (the integer). For the sake of simplicity, lets assume the cabin number is not useful, but that the cabin area is. This is a reasonable assumption because the cabin area is likely to have a large effect on the location of the passenger, whereas the cabin number is less likely to have a large effect.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\ndef getCabinLetter(cabin: str):\n    if cabin is None or not isinstance(cabin,str): return None\n    result =  re.search(\"^[a-zA-Z]*\",cabin)\n    if not result: return None\n    return result[0]\n    \ncabinLetter = trainData[\"Cabin\"].apply(getCabinLetter)\n\nprint(\"Cabin Letters\")\nprintUniqueCounts(cabinLetter)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see there are only 8 valid options for cabin letter. Being a small number this is a good candidate for one-hot encoding, again solving the missing data issue.","metadata":{}},{"cell_type":"code","source":"trainData[\"Cabin\"] = cabinLetter\ntrainData = pd.get_dummies(trainData,columns=[\"Cabin\"])\ntrainData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That covers all the missing data, so now we just need to deal with non-numeric data. Lets start by converting Male/Female and True/False to 0/1","metadata":{}},{"cell_type":"code","source":"trainData[\"Sex\"] = trainData[\"Sex\"]==\"male\"\ntrainData = trainData*1\ntrainData.head()\nprintUniqueCounts(cabinLetter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Name and Ticket remain as non-numeric data. We probably could do some NLP to make useful inferences out of the names, but that would be a lot of work and likely not yield a big difference in results. Ticket is also likely not to be very useful info. So lets just delete both columns.","metadata":{}},{"cell_type":"code","source":"trainData = trainData.drop(columns=[\"Name\",\"Ticket\",\"PassengerId\"])\ntrainData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally lets do a little bit of feature engineering. Most of these features look pretty useful as is, but Sibsp and Parch stand out as being a potentially useful target for a little bit of engineering. \nSibSp: siblings and spouses\nParch: parents and children\n\nWe can combine these two to get family size. Total family size might be an important metric so lets make an additional column for family size ","metadata":{}},{"cell_type":"code","source":"\ntrainData.insert(len(trainData.columns),\"FamSize\",trainData[\"SibSp\"]+trainData[\"Parch\"])\ntrainData.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we have to normalize the data. Lets use zscore normalization on Age and Fare:\n**temporarily dropping Age to fix bugs**","metadata":{}},{"cell_type":"code","source":"from scipy.stats import zscore\ncolsToNormalize = [\"Age\",\"Fare\"]\ntrainData[colsToNormalize] = trainData[colsToNormalize].apply(zscore)\n\n#drop age for now\n\ntrainDataY = trainData[\"Survived\"]\ntrainDataX = trainData.drop(columns=[\"Survived\",\"Age\"])\ntrainDataX.head()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have prepared the data for our ML algorithm.\n\nLets create a model with 3 hidden layers, of 20, 8 and 5 neurons. Each hidden layer will have a ReLU activation function. The final output neuron will have a sigmoid activation function.\nFor the loss function we will use binary cross entropy.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"inCount = trainDataX.shape[1]#number of inputs\nlayerWeights = []\nlayerBiases = []\n\nrelu = nn.ReLU()\nsigmoid = nn.Sigmoid()\nlayers = [[20,relu],[8,relu],[5,relu],[1,sigmoid]]#[node count, activation function]\nloss = nn.BCELoss()\nlearningRate = 0.01\n\nlastNodeCount = inCount\nfor layer in layers:\n    nodeCount = layer[0]\n    layerWeights.append(torch.rand((lastNodeCount,nodeCount),requires_grad=True))\n    layerBiases.append(torch.rand(nodeCount,requires_grad=True))\n    lastNodeCount = nodeCount\n\n\ndef forward(X):\n    A = X\n    for i in range(len(layers)):\n        layer = layers[i]\n        function = layer[1]\n        prod = torch.matmul(A,layerWeights[i])\n        z = prod + layerBiases[i]\n        A = function(z)\n        \n        \nX = torch.tensor(trainDataX.values,dtype=torch.float32)\nY = torch.tensor(trainDataY.values,dtype=torch.float32)\n\nlosses = []\niterations = 100\nfor epoch in range(iterations):\n    print(epoch)\n    yHat = forward(X)\n    print(yHat)\n    print(yHat)\n    l = loss(Y,yHat)\n    l.backward()\n    \n    with torch.no_grad():\n        layerWeights -= learningRate*layerWeights.grad\n        layerBiases -= learningRate*layerBiases.grad\n    \n    layerWeights.grad.zero_()\n    layerBiaseds.grad.zero_()\n    losses.append(l)\n\n\nprint(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}